### model
model_name_or_path: Qwen/Qwen2.5-VL-7B-Instruct
image_max_pixels: 2097152 # 1024*1024*2
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 32
lora_target: all

### dataset
dataset_dir: PaCo-Dataset
dataset: consistency_reward_cot_train, sharegpt_editing_cot
eval_dataset: consistency_reward_cot_test
template: qwen2_vl
cutoff_len: 16384
max_samples: 100000
overwrite_cache: true
preprocessing_num_workers: 32
dataloader_num_workers: 32

### train
first_token_weight: 0.1
per_device_train_batch_size: 8 # For 80GB GPU, batch_size=8
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
num_train_epochs: 2.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
val_size: 0
per_device_eval_batch_size: 8
eval_strategy: epoch
compute_accuracy: true
first_token_accuracy: true

### generation
do_sample: true
temperature: 0.7
top_p: 0.9
max_new_tokens: 1024

### output
output_dir: ./checkpoints/paco-reward-7b_lora_sft # Modify as needed
logging_steps: 10
save_strategy: epoch
save_total_limit: null # unlimited
plot_loss: true
overwrite_output_dir: true
save_only_model: true
report_to: none  # choices: [none, wandb, tensorboard, swanlab, mlflow]
run_name: PaCo-Reward-Qwen2.5-SFT-LoRA-CoT
project: PaCo-Reward